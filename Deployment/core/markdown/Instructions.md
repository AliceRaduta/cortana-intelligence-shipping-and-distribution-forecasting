## Your solution has been successfully deployed!

The basic architecture of the solution is shown below:   

[![Solution diagram]({PatternAssetBaseUrl}/dbschema_sourcediagram.PNG)]({PatternAssetBaseUrl}/dbschema_sourcediagram.PNG)

The diagram shows the ML webservice in the center, taking HistoricalOrders and ForecastParameter tables as input and generating StatisticalForecast and ForecastHistory tables as output. By default, the Data Factory will start running forecasts for the past few months of data as soon as it is deployed. Connect to the SQL Server or view the Solution Dashboard to see the created forecasts. The Data Factory pipelines are, by default, in the active state until the end of the current month. Follow the Data Factory link above to configure the pipelines as necessary.     

The main components deployed by the solution are:
	
* An [Azure Data Factory]({Outputs.dataFactoryUrl}) is used for orchestrating forecast generation and bulk loading of forecasts into a SQL Server database on a monthly schedule. You can monitor the data pipelines by clicking the provided link.

* An [Azure SQL Server]({Outputs.sqlServerUrl}) is used for persistent storage of historic demand and forecasts. You can use the SQL server and database name showing on the last page of deployment with the username and password that you set up in the beginning of your deployment to log in your database and check the results.

* An [Azure Machine Learning Webservice]({Outputs.amlWebServiceUrl}) is used for generating demand forecasts. You can view your forecasting model on machine learning experiment by navigating to your Machine Learning Workspace. The machine learning model is deployed as Azure Web Service to be invoked by the Azure Data Factory. You can view your the web service API manual by clicking the link on your deployment page.

* A [Power BI Dashboard]({Outputs.solutionDashboardUrl}) - Once the solution is deployed, the forecasting web service generates forecasts for a simulated sample data set, included with this solution. To view the generated forecasts, and drill down by customer, product, and destination, you can click on the provided link. Note that this dashboard will be blank until the deployed forecasting pipeline is completed (approximately 10-15 minutes). 

## Next Steps

### **Check out Technical Solution Guide**

For detailed information about the solution, go to our [GitHub repository](https://github.com/Azure/cortana-intelligence-shipping-and-distribution-forecasting). In particular, check out the [**Technical Solution Guide**](https://github.com/Azure/cortana-intelligence-shipping-and-distribution-forecasting/blob/master/Technical%20Deployment%20Guide/Technical-Solution-Guide.md) hosted on that repository, which contains detailed information about the solution, such as components involved, valid data schema, sample data generation code, etc.

### **View Power BI Dashboard**

[Power BI Dashboard]({Outputs.solutionDashboardUrl}) created as a part of the deployment is a very good first step to understanding the data and the generated forecasts. The dashboard contains simulated data set that is included with this solution. More information about the data set and more generally about the data schema, can be found in the Technical Solution Guide available with this solution. To view the generated forecasts, and drill down into the forecasted demand by customer, product, and destination, follow the [Power BI Dashboard link]({Outputs.solutionDashboardUrl}). 

>NOTE: Power BI dashboard will initially be blank until the deployed forecasting pipeline is completed (approximately 10 minutes). Make sure to refresh the page at that time, in order to view the results in the Power BI Dashboard.

### **Connect to the Azure SQL DB and view the data**

If you would like to take a look at the simulated dataset included with this solution, you can use a SQL Server Management Studio (SSMS) to do that. SSMS is an integrated environment for managing any SQL infrastructure. [This document](https://docs.microsoft.com/en-us/azure/sql-database/sql-database-connect-query-ssms) demonstrates how to use SSMS to connect to an Azure SQL database, and then use Transact-SQL statements to query, insert, update, and delete data in the database. 

### **Load new simulated data into the Azure SQL DB**

The solution comes with an example demand data set pre-loaded into the provisioned SQL database. This example data set is generated by an R script, and loaded into the SQL table containing historical demand during the deployment process. The R script simulates grouped time series over a number of customers, products and destinations, and can be found [here](https://raw.githubusercontent.com/Azure/cortana-intelligence-shipping-and-distribution-forecasting/master/Technical%20Deployment%20Guide/ADF/db/ExampleDataGen.R?token=AOWz9tECHx4w7XUuJFH7QpF0xphsblu-ks5ZVpacwA%3D%3D).

>NOTE: These instructions assume you have [R](https://www.r-project.org/) installed on your machine.

**Generating new simulated data**

To load new data, you need to modify the following lines in the [ExampleDataGen.R](https://raw.githubusercontent.com/Azure/cortana-intelligence-shipping-and-distribution-forecasting/master/Technical%20Deployment%20Guide/ADF/db/ExampleDataGen.R?token=AOWz9tECHx4w7XUuJFH7QpF0xphsblu-ks5ZVpacwA%3D%3D) script to include the SQL Server credentials you generated as a part of the deployment.

```bash
# SQL Server credentials
myServer <- "<server>.database.windows.net"
myUser <- "<user>"
myPassword <- "<passwd>"
```

To generated different data groups or hierarchies, you can modify the following variables:

```bash
# Default Hierarchy 
customerList <- c("Contoso","Protoso")
productCategories <- c("Plastics","Metals")
destinationList <- c("China","United States","India")
```

Once you are done modifying the R script, you can open any R IDE or an R console and execute the script by typing:

```bash
setwd("<rscript_directory_path>")
source("ExampleDataGen.R")
```

where the *<rscript_directory_path>* refers to the directory where you downloaded the ExampleDataGen.R script.

**Resetting Azure Data Factory pipelines**

After the new data is loaded into the SQL database, you will need to reset the Azure Data Factory to re-run the demand forecast pipelines on the new data. You can do this from Azure portal, by going to the deployed Azure Data Factory and resetting resetting the pipelines manually. However, we provided a powershell script, [reset_slices.ps1](https://github.com/Azure/cortana-intelligence-shipping-and-distribution-forecasting/tree/master/Technical%20Deployment%20Guide/ADF/db), that lets you reset the ADF pipelines automatically.

To execute the script, download it to your local machine, open Windows PowerShell, and run the following command:

```bash
cd <psscript_directory_path>
.\reset_slices.ps1 <subscription_id> <resource_group_name> <data_factory_name>
```

where *psscript_directory_path* is the location of the PowerShell script, *subscription_id* is Azure subscription ID, *resource_group_name* is Resource Group name of the solution, and *data_factory_name* is the name of the Azure Data Factory deployed by the solution.

### **Load your own data into the Azure SQL DB**

If you would like to generate forecasts on your own hierarchical demand data, you can do that by loading your data into the Azure SQL database created by the solution. There are many ways to do that, but here we highlight two ways to loading data into the Azure DB, one using bcp command and the other way using Azure Data Factory. 

* [Load data from CSV into Azure SQL Database (flat files)](https://docs.microsoft.com/en-us/azure/sql-database/sql-database-load-from-csv-with-bcp)

    You can use the bcp (bulk copy) command-line utility to import data from a CSV file into Azure SQL Database. [This document](https://docs.microsoft.com/en-us/azure/sql-database/sql-database-load-from-csv-with-bcp) describes how to that from creating the destination table (which is already created in the solution), to creating a source data file, to loading the data using the bcp command-line utility.

* [Copy data from Blob Storage to SQL Database using Azure Data Factory](https://docs.microsoft.com/en-us/azure/data-factory/data-factory-copy-data-from-azure-blob-storage-to-sql-database)

    You can also create a data factory with a pipeline to copy data from Blob storage to SQL database. The Copy Activity performs the data movement in Azure Data Factory. It is powered by a globally available service that can copy data between various data stores in a secure, reliable, and scalable way. [This document](https://docs.microsoft.com/en-us/azure/data-factory/data-factory-copy-data-from-azure-blob-storage-to-sql-database) walks through a simple example of copying some data from Azure blob to Azure SQL table.

Note that after the new data is loaded into the SQL database, you will need to reset the Azure Data Factory to re-run the demand forecast pipelines on the new data. You can do so by following the steps outlined in the previous section, under *Resetting Azure Data Factory pipelines*.


